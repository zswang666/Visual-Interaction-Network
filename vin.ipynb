{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vin.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "d40dk0M37psU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Visual Interaction Network\n",
        "From just a glance, humans can infer not only what and where objects are, but also what will happen to them over the upcoming seconds, minutes or even longer in some cases. For example, if you kick a football against a wall, your brain predicts what will happen when the ball hits the wall and their movement afterward. These predictions are guided by a sophisticated cognitive system for reasoning about objects and their mutual interactions.\n",
        "\n",
        "Visual interaction network is a general-purpose model for learning the dynamics of a physical system from raw visual observations. The model consists of a perceptual front-end based on convolutional neural networks and a dynamics predictor based on interaction networks.  The following shows the comparison between the ground truth and the prediction from visual interaction network.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/deepmind-live-cms/documents/ezgif.com-optimize%2520%25281%2529.gif\" width=\"600\"/>"
      ]
    },
    {
      "metadata": {
        "id": "E9GO5SbWEGfl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "The **V**isual **I**nteraction **N**etwork (**VIN**) learns to produce future trajectories of objects in a physical system from only a few video frames.\n",
        "- **Visual Encoder** takes a triplet of frames as input and outputs a state code. A state code is a list of vectors, one for each object in the scene. The encoder is applied in a sliding window fashion over a sequence of frames, producing a sequence of state codes.\n",
        "- **Dynamics Predictor** takes a sequence of state codes (output from the visual encoder applied in a sliding-window manner from a sequence of frames) and predicts a candidate state code for the next frame. The dynamics predictor rolls the states forward in time by computing their interactions and dynamics, generating states of future timestep.\n",
        "- **State Decoder** converts a state code to a output state, which has physical meanings and in our case, is a list of each objects's position and velocity vector.\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/isKXEwm.png\" width=\"600\">"
      ]
    },
    {
      "metadata": {
        "id": "CffI5ntRPsQc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f106ce19-f6f2-4362-d4d5-78cd15d27311",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1533394499689,
          "user_tz": -480,
          "elapsed": 5327,
          "user": {
            "displayName": "Tsun Hsuan Wang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112844842626206939059"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/zswang666/Visual-Interaction-Network/raw/master/gravity_dataset.tar.gz\n",
        "!tar zxf gravity_dataset.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-08-04 14:54:57--  https://github.com/zswang666/Visual-Interaction-Network/raw/master/gravity_dataset.tar.gz\r\n",
            "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/zswang666/Visual-Interaction-Network/master/gravity_dataset.tar.gz [following]\n",
            "--2018-08-04 14:54:57--  https://raw.githubusercontent.com/zswang666/Visual-Interaction-Network/master/gravity_dataset.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1132019 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘gravity_dataset.tar.gz’\n",
            "\n",
            "gravity_dataset.tar 100%[===================>]   1.08M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2018-08-04 14:54:58 (9.42 MB/s) - ‘gravity_dataset.tar.gz’ saved [1132019/1132019]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NxZnzKYyrC2Q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "9df0c503-b553-4687-c72b-47844ec8177c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1533396140958,
          "user_tz": -480,
          "elapsed": 575,
          "user": {
            "displayName": "Tsun Hsuan Wang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112844842626206939059"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Import Module \"\"\"\n",
        "import os\n",
        "import time\n",
        "import collections\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "%pylab inline\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['axes']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "FRo9_C2r7ztw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ]
    },
    {
      "metadata": {
        "id": "xqEoPnOXkuJb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Constant \"\"\"\n",
        "MODE = 'test' # train / test\n",
        "PSEUDO = False # True for using psuedo inputs (no data); False for dataset\n",
        "\n",
        "HEIGHT = 32 # image (frame) height\n",
        "WIDTH = 32 # image width\n",
        "N_CHANNELS = 4 # image channel\n",
        "N_OBJECTS = 3 # number of objects in a scene\n",
        "N_FRAMES = 6 # number of frames to be sent as inputs\n",
        "FUTURE_N_STEPS = 8 # how far away from now we are predicting in training\n",
        "IMG_DIR = './gravity_dataset/img/' # dataset: image directory\n",
        "DATA_DIR = './gravity_dataset/data/' # dataset: data directory that stores labels\n",
        "N_SETS = 10 # number of image sets (sequences)\n",
        "SIM_FRAMES = 50 # number of saved frames per simulation\n",
        "\n",
        "N_FILTERS = 128 # number of filters in Visual Encoder\n",
        "STATE_DIMS = 64 # dimension of state codes\n",
        "VE_STATE_DIMS = 64 # state dimensions of Visual Encoder\n",
        "DP_STATE_DIMS = 64 # state dimensions of Dynamics Predictor\n",
        "CORE_STATE_DIMS = 64 # state dimensions of cores\n",
        "OUTPUT_DIMS = 4 # output state dimensions (position / velocity vectors)\n",
        "TEMPORAL_OFFSETS = [1, 3, 4] # must be subset of (range(N_FRAMES - 2) + 1)\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 0.0005\n",
        "ROLLOUT_NUM = 20\n",
        "MAX_EPOCHS = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nx1rofNW738E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Layer"
      ]
    },
    {
      "metadata": {
        "id": "BAu1_oikqd3n",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Tensorflow Layer \"\"\"\n",
        "def _get_variable(name, shape, initializer=None):\n",
        "    return tf.get_variable(name, shape, initializer=initializer)\n",
        "\n",
        "\n",
        "def Linear(scope, inputs, num_outputs, initializer=None, bias_zero_init=True,\n",
        "           activation_fn=tf.nn.relu, skip_connection=False):\n",
        "    with tf.variable_scope(scope):\n",
        "        # specify weights and biases\n",
        "        N = inputs.get_shape()[-1].value\n",
        "        weight = _get_variable('weights',\n",
        "                               shape=[N, num_outputs],\n",
        "                               initializer=initializer)\n",
        "        if bias_zero_init:\n",
        "            biases = _get_variable('biases', [num_outputs],\n",
        "                                   tf.constant_initializer(0.0))\n",
        "        else: # NOTE: non-zero initializer for bias\n",
        "            biases = _get_variable('biases', [num_outputs], None)\n",
        "        # perform fully connected layer\n",
        "        outputs = tf.nn.bias_add(tf.matmul(inputs, weight), biases)\n",
        "        # skip connection\n",
        "        if skip_connection:\n",
        "            outputs = outputs + inputs\n",
        "        # perform activation\n",
        "        if activation_fn is not None:\n",
        "            outputs = activation_fn(outputs)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def Conv2d(scope, inputs, num_output_channels, kernel_size=[3, 3], stride=[1, 1], padding='SAME',\n",
        "           initializer=None, activation_fn=tf.nn.relu, skip_connection=False):\n",
        "    with tf.variable_scope(scope):\n",
        "        # specify weights and biases\n",
        "        B, H, W, C = inputs.get_shape().as_list()\n",
        "        kernel_h, kernel_w = kernel_size\n",
        "        if initializer is None: # NOTE: special initializer used for convolution\n",
        "            d = 1.0 / np.sqrt(C * kernel_h * kernel_w)\n",
        "            initializer = tf.random_uniform_initializer(minval=-d, maxval=d)\n",
        "        kernel_shape = [kernel_h, kernel_w, C, num_output_channels]\n",
        "        kernel = _get_variable('weights',\n",
        "                               shape=kernel_shape,\n",
        "                               initializer=initializer)\n",
        "        biases = _get_variable('biases', [num_output_channels],\n",
        "                               initializer=initializer)\n",
        "        # perform 2D convolution\n",
        "        stride_h, stride_w = stride\n",
        "        outputs = tf.nn.conv2d(inputs, kernel, strides=[1, stride_h, stride_w, 1], padding=padding)\n",
        "        outputs = tf.nn.bias_add(outputs, biases)\n",
        "        # skip connection\n",
        "        if skip_connection:\n",
        "            outputs = outputs + inputs\n",
        "        # perform activation\n",
        "        if activation_fn is not None:\n",
        "            outputs = activation_fn(outputs)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def MaxPool2d(scope, inputs, kernel_size=[2, 2], stride=[2, 2], padding='SAME'):\n",
        "    with tf.variable_scope(scope):\n",
        "        kernel_h, kernel_w = kernel_size\n",
        "        stride_h, stride_w = stride\n",
        "        outputs = tf.nn.max_pool(inputs,\n",
        "                                 ksize=[1, kernel_h, kernel_w, 1],\n",
        "                                 strides=[1, stride_h, stride_w, 1],\n",
        "                                 padding=padding,\n",
        "                                 name='max-pool2d')\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bzjbSPEZ76-v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataloader\n",
        "### Inputs:\n",
        "- **frames**: a sequence of frames of shape (```HEIGHT x WIDTH x N_CHANNELS```) with length ```N_FRAMES```.\n",
        "- **x_coor, y_coor**: a sequence of *x, y* coordinates of shape (```HEIGHT x WIDTH x 1```) with length ```N_FRAMES```.\n",
        "- **discount_factor**: a factor to discount the  prediction loss in further future steps.\n",
        "\n",
        "### Labels:\n",
        "- **current_label**: object states of shape (```N_OBJECTS x OUTPUT_DIMS```) of the input sequence. Note that the length is ```N_FRAMES - 2``` because we are using triplet while applying sliding window over the input sequence.\n",
        "- **future_label**: predicted object states of shape (```N_OBJECTS x OUTPUT_DIMS```) in the future ```FUTURE_N_STEPS```.\n",
        "\n",
        "\n",
        "If you set ```PSEUDO = True```, you are using placeholder as input layer, which specifies the shape of each inputs and labels. Set to ```True``` for debugging tensor shape consistency."
      ]
    },
    {
      "metadata": {
        "id": "7EakiL3k9-ga",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Dataloader \"\"\"\n",
        "class GravityDataset(object):\n",
        "    def __init__(self, mode):\n",
        "        assert mode in ['train', 'test'], \\\n",
        "               'Arguments \\'mode\\' of GravityDataset must be \\'train\\'/\\'test\\''\n",
        "        self._mode = mode\n",
        "        self.x_coor = None\n",
        "        self.y_coor = None\n",
        "        self._load_data(mode)\n",
        "        self.frames, self.current_label, self.future_label = self._create()\n",
        "\n",
        "    def _load_data(self, split):\n",
        "        if split == 'train':\n",
        "            n_sets = N_SETS\n",
        "        else:\n",
        "            n_sets = 1\n",
        "        # read all images in the data split\n",
        "        images = np.zeros((n_sets, SIM_FRAMES, HEIGHT, WIDTH, N_CHANNELS), dtype=np.float32)\n",
        "        for i in range(n_sets):\n",
        "            for j in range(SIM_FRAMES):\n",
        "                img_path = os.path.join(IMG_DIR, split, '{}_{}.png'.format(i, j))\n",
        "                images[i, j] = mpimg.imread(img_path)[:, :, :N_CHANNELS]\n",
        "\n",
        "        # real all labels in the data split\n",
        "        labels = np.zeros((n_sets, SIM_FRAMES, N_OBJECTS * 5), dtype=np.float32)\n",
        "        for i in range(n_sets):\n",
        "            f_path = os.path.join(DATA_DIR, split, '{}.csv'.format(i))\n",
        "            f = open(f_path, 'r')\n",
        "            labels[i] = [line[:-1].split(',') for line in f.readlines()]\n",
        "        labels = np.reshape(labels, (n_sets, SIM_FRAMES, N_OBJECTS, 5))\n",
        "\n",
        "        # prepare input data and corresponding labels\n",
        "        n_valid_seqs = SIM_FRAMES - N_FRAMES - FUTURE_N_STEPS + 1\n",
        "        input_images = np.zeros((n_sets * n_valid_seqs, N_FRAMES, HEIGHT, WIDTH, N_CHANNELS), dtype=np.float32)\n",
        "        current_labels = np.zeros((n_sets * n_valid_seqs, N_FRAMES - 2, N_OBJECTS, OUTPUT_DIMS), dtype=np.float32)\n",
        "        future_labels = np.zeros((n_sets * n_valid_seqs, FUTURE_N_STEPS, N_OBJECTS, OUTPUT_DIMS), dtype=np.float32)\n",
        "        for i in range(n_sets):\n",
        "            for j in range(n_valid_seqs):\n",
        "                input_images[i * n_valid_seqs + j] = images[i, j:(j + N_FRAMES)]\n",
        "                tmp = labels[i, (j + N_FRAMES - 4):(j + N_FRAMES)]\n",
        "                tmp = np.reshape(tmp, (N_FRAMES - 2, N_OBJECTS, 5))\n",
        "                current_labels[i * n_valid_seqs + j] = tmp[:, :, 1:5]\n",
        "                tmp = labels[i, (j + N_FRAMES):(j + N_FRAMES + FUTURE_N_STEPS)]\n",
        "                tmp = np.reshape(tmp, (FUTURE_N_STEPS, N_OBJECTS, 5))\n",
        "                future_labels[i * n_valid_seqs + j] = tmp[:, :, 1:5]\n",
        "\n",
        "        # x, y coordinate\n",
        "        x = np.linspace(0, 1, WIDTH)\n",
        "        y = np.linspace(0, 1, HEIGHT)\n",
        "        xx, yy = np.meshgrid(x, y)\n",
        "        self.x_coor = np.stack([np.expand_dims(xx, -1)] * BATCH_SIZE * (N_FRAMES - 1), 0)\n",
        "        self.y_coor = np.stack([np.expand_dims(yy, -1)] * BATCH_SIZE * (N_FRAMES - 1), 0)\n",
        "\n",
        "        # output data\n",
        "        self._input_images = tf.convert_to_tensor(input_images)\n",
        "        self._current_labels = tf.convert_to_tensor(current_labels)\n",
        "        self._future_labels = tf.convert_to_tensor(future_labels)\n",
        "        self._dataset_size = input_images.shape[0]\n",
        "\n",
        "    def _create(self):\n",
        "        DataFormat = collections.namedtuple('Data', 'frames, current_label, future_label')\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((self._input_images,\n",
        "                                                      self._current_labels,\n",
        "                                                      self._future_labels))\n",
        "        if self._mode == 'train': # do shuffle\n",
        "            dataset = dataset.shuffle(buffer_size=self._dataset_size)\n",
        "            batch_size = BATCH_SIZE\n",
        "        else:\n",
        "            batch_size = 1\n",
        "        dataset = dataset.repeat()\n",
        "        dataset = dataset.batch(BATCH_SIZE)\n",
        "        iterator = dataset.make_one_shot_iterator()\n",
        "        batch_image, batch_cur_label, batch_fut_label = iterator.get_next()\n",
        "\n",
        "        return DataFormat(frames=batch_image,\n",
        "                          current_label=batch_cur_label,\n",
        "                          future_label=batch_fut_label)\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return self._dataset_size\n",
        "\n",
        "\n",
        "if not PSEUDO:\n",
        "    dset = GravityDataset(MODE)\n",
        "    frames, current_label, future_label = dset.frames, dset.current_label, dset.future_label\n",
        "    frames_list = tf.unstack(frames, N_FRAMES, 1)\n",
        "    current_label_list = tf.unstack(current_label, N_FRAMES - 2, 1)\n",
        "    future_label_list = tf.unstack(future_label, FUTURE_N_STEPS, 1)\n",
        "    discount_factor = tf.placeholder(tf.float32, [], name='discount_factor')\n",
        "    x_coor = tf.placeholder(tf.float32, [None, HEIGHT, WIDTH, 1], name='x_coordinate')\n",
        "    y_coor = tf.placeholder(tf.float32, [None, HEIGHT, WIDTH, 1], name='y_coordinate')\n",
        "else:\n",
        "    # consecutive frames\n",
        "    frames = tf.placeholder(tf.float32, [None, N_FRAMES, HEIGHT, WIDTH, N_CHANNELS], name='frames')\n",
        "    frames_list = tf.unstack(frames, N_FRAMES, 1)\n",
        "    # ground truth of current states\n",
        "    current_label = tf.placeholder(tf.float32, [None, N_FRAMES - 2, N_OBJECTS, OUTPUT_DIMS], name='current_label')\n",
        "    current_label_list = tf.unstack(current_label, N_FRAMES - 2, 1)\n",
        "    # ground truth of future states\n",
        "    future_label = tf.placeholder(tf.float32, [None, FUTURE_N_STEPS, N_OBJECTS, OUTPUT_DIMS], name='future_label')\n",
        "    future_label_list = tf.unstack(future_label, FUTURE_N_STEPS, 1)\n",
        "    # discount factor\n",
        "    discount_factor = tf.placeholder(tf.float32, [], name='discount_factor')\n",
        "    # x, y coordinates\n",
        "    x_coor = tf.placeholder(tf.float32, [None, HEIGHT, WIDTH, 1], name='x_coordinate')\n",
        "    y_coor = tf.placeholder(tf.float32, [None, HEIGHT, WIDTH, 1], name='y_coordinate')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3mm7tGzh7-7e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visual Encoder\n",
        "The **V**isual **E**ncoder (**VE**) is a CNN that produces a state code from a sequence of 3 images. \n",
        "\n",
        "**A frame pair encoder (IPE)** is applied to both consecutive pairs of frames in a sequence of 3 frames. It first <font color='red'>concatenate a pair of frames with a constant *x, y* coordinate channels</font> and <font color='red'>perform convolution and max-pooling alternatingly until unit width and height</font>.  The inclusion of constant coordinate channels allows positions to be incorporated throughout much of the processing. Also note that weight sharing is applied to approximates a <font color='red'>temporal convolution over the input sequence</font>.\n",
        "\n",
        "**A slot-wise MLP** is then applied to the pair code to <font color='red'>produce state code of shape $N_{object}\\times L_{code}$</font>, where $N_{object}$ is the number of objects in the scene and $L_{code}$ is the length of each state code slot.\n",
        "\n",
        "Finally, we further encode two pairwise state code to a triplet with a sequence of fully-connected layers. Weight sharing is also applied here to approximates temporal convolution. The visual encoder is trained by a reconstruction loss applied to the decoded output of the visual encoder in the beginning of the sequence.\n",
        "\n",
        "<img src=\"https://i.imgur.com/C0DEgje.png\" width=\"500\">"
      ]
    },
    {
      "metadata": {
        "id": "QEQrgsuHmLRE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Visual Encoder \"\"\"\n",
        "def IPE(fpairs_list, x, y):\n",
        "    with tf.variable_scope('IPE'):\n",
        "        ##############################################################################################\n",
        "        ###                    Temporal convolution approximated using weight sharing              ###\n",
        "        ##############################################################################################\n",
        "        fpairs = tf.concat(fpairs_list, 0)\n",
        "        inputs = tf.concat([fpairs, x, y], 3)\n",
        "\n",
        "        h_0 = MaxPool2d('mp_0', Conv2d('h_0', inputs, N_FILTERS))\n",
        "        h_1 = MaxPool2d('mp_1', Conv2d('h_1', h_0, N_FILTERS, skip_connection=True))\n",
        "        h_2 = MaxPool2d('mp_2', Conv2d('h_2', h_1, N_FILTERS, skip_connection=True))\n",
        "        h_3 = MaxPool2d('mp_3', Conv2d('h_3', h_2, N_FILTERS, skip_connection=True))\n",
        "        h_4 = MaxPool2d('mp_4', Conv2d('h_4', h_3, N_FILTERS, skip_connection=True))\n",
        "\n",
        "        # separate along time axis\n",
        "        pairs = tf.reshape(h_4, [-1, N_FILTERS])\n",
        "        p_list = []\n",
        "        for i in range(N_FRAMES - 1):\n",
        "            p_list.append(tf.slice(pairs, [BATCH_SIZE * i, 0], [BATCH_SIZE, -1]))\n",
        "        ##############################################################################################\n",
        "        ###                                  End of your code                                      ###\n",
        "        ##############################################################################################\n",
        "\n",
        "    return p_list\n",
        "\n",
        "\n",
        "def VE(f_list, x, y):\n",
        "    with tf.variable_scope('VE'):\n",
        "        ### Construct Temporal Pairs ###\n",
        "        # concat two consecutive frames as a pair and encode\n",
        "        fpairs_list = []\n",
        "        for i in range(len(f_list) - 1):\n",
        "            fpairs_list.append(tf.concat([f_list[i], f_list[i+1]], 3))\n",
        "        pairs_list = IPE(fpairs_list, x, y)\n",
        "        pairs = tf.concat(pairs_list, 0)\n",
        "\n",
        "        # temporal convolution approximated by fully-connected layer with weight sharing\n",
        "        h_0 = Linear('h_0', pairs, N_OBJECTS * STATE_DIMS, activation_fn=None, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
        "        enpairs_list = []\n",
        "        for i in range(N_FRAMES - 1):\n",
        "            enpair = tf.slice(h_0, [BATCH_SIZE * i, 0], [BATCH_SIZE, -1])\n",
        "            enpair = tf.reshape(enpair, [-1, N_OBJECTS, STATE_DIMS])\n",
        "            enpairs_list.append(enpair)\n",
        "\n",
        "        ### Construct Temporal Triplets ###\n",
        "        # concat two consecutive pairs as a triplet and encode\n",
        "        triplets_list = []\n",
        "        for i in range(N_FRAMES - 2):\n",
        "            triplets_list.append(tf.concat([enpairs_list[i], enpairs_list[i+1]], 2))\n",
        "        triplets = tf.concat(triplets_list, 0)\n",
        "        triplets = tf.reshape(triplets, [-1, STATE_DIMS * 2])\n",
        "\n",
        "        h_1 = Linear('h_1', triplets, VE_STATE_DIMS, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
        "        h_2 = Linear('h_2', h_1, VE_STATE_DIMS, skip_connection=True, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
        "        h_3 = Linear('h_3', h_2, VE_STATE_DIMS, skip_connection=True, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
        "\n",
        "        # separate along time axis\n",
        "        encoded_triplets = tf.reshape(h_3, [-1, N_OBJECTS, STATE_DIMS])\n",
        "        s_list = []\n",
        "        for i in range(N_FRAMES - 2):\n",
        "            s_list.append(tf.slice(encoded_triplets, [BATCH_SIZE * i, 0, 0], [BATCH_SIZE, -1, -1]))\n",
        "\n",
        "    return s_list\n",
        "\n",
        "\n",
        "sliding_window_list = VE(frames_list, x_coor, y_coor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7QnAEu9Y8BnV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dynamics Predictor and Rollout\n",
        "The **D**ynamic **P**redictor (**DP**) is a variant of an **I**nteraction **N**etwork (**IN**), a state-to-state physical predictor model.  For each time offset *t*, given an input state code sequence (output from VE), a core of DP is an IN which takes objects dynamics and their interaction into consideration to predict future state code. The main difference between DP and a vanilla IN is aggregation over multiple temporal offsets. The temporal offset aggregation enhances its power by allowing it to accommodate both fast and slow movements by different objects within a sequence of frames.\n",
        "\n",
        "<img src=\"https://i.imgur.com/3qCP4lf.png\" width=\"400\">\n",
        "\n",
        "**Interaction Network**\n",
        "<font color=\"magenta\">GGGGGGGGGGGGGGGGGGGGGGGGGG</font>\n",
        "\n",
        "<img src=\"https://i.imgur.com/159L98K.png\" width=\"800\">\n",
        "\n",
        "**Self-Dynamics Module** models the dynamics of an objects itself. It is composed of <font color=\"red\">2 fully-connected layers that convert state code for each object in each time step</font>. The output dimensions of the 2 fully-connected layers are both ```CORE_STATE_DIMS```.\n",
        "\n",
        "**Relation Module** models the relation of a pair of objects. It <font color=\"red\">considers the potential relations between all object pairs</font>. For a pair in all *permutations*, <font color=\"red\">state codes of the two objects are concatenated to form an input pair</font>, which will be subsequently sent into relation network. The relation network consists of 3 fully-connected layer with output dimension as ```CORE_STATE_DIMS```. Finally, we <font color=\"red\">accumulate all the effects over a specific object</font>, outputing a state code of shape (```N_OBJECTS x CORE_STATE_DIMS```).\n",
        "\n",
        "<img src=\"https://i.imgur.com/1NQQR2q.png\" width=\"800\">\n",
        "\n",
        "The results of self-dynamics module and relation module are summed and post-processed by the affector. The affector output is further concatenated to the input state sequence and sent to the output module to produce the predicted slot.\n",
        "\n",
        "**Rollout** is performed by recursively sending a rolling state sequence to DP to predict state code in future steps. The rolling process of the state sequence is to <font color=\"red\">abandon the oldest state and concatenate the upcoming predicted future state to the state sequence</font>. This can be implemented using function ```tf.scan()``` ([TensorFlow Scan Examples](https://rdipietro.github.io/tensorflow-scan-examples/)).\n",
        "\n",
        "<img src=\"https://i.imgur.com/OS04r29.png\" width=\"350\">\n"
      ]
    },
    {
      "metadata": {
        "id": "AiZNoNW0GRMf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Dynamics Predictor and Rollout \"\"\"\n",
        "def core(state, index):\n",
        "    with tf.variable_scope('core_{}'.format(index)):\n",
        "        ##############################################################################################\n",
        "        ###                           Implement self-dynamics module                               ###\n",
        "        ##############################################################################################\n",
        "        ### Self-Dynamics Module ###\n",
        "        with tf.variable_scope('self_dynamics'):\n",
        "            SD_inputs = tf.reshape(state, [-1, STATE_DIMS])\n",
        "            h_1 = Linear('h_1', SD_inputs, CORE_STATE_DIMS, bias_zero_init=False)\n",
        "            h_2 = Linear('h_2', h_1, CORE_STATE_DIMS, skip_connection=True, bias_zero_init=False, activation_fn=None)\n",
        "            SD_outputs = tf.reshape(h_2, [-1, N_OBJECTS, CORE_STATE_DIMS])\n",
        "\n",
        "        ##############################################################################################\n",
        "        ###                                  End of your code                                      ###        \n",
        "        ##############################################################################################\n",
        "        \n",
        "        ##############################################################################################\n",
        "        ###                              Implement relation module                                 ###\n",
        "        ##############################################################################################\n",
        "        ### Relation Module ###\n",
        "        n_relations = int(N_OBJECTS * (N_OBJECTS - 1))\n",
        "        with tf.variable_scope('relation_module'):\n",
        "            # collect object pairs as inputs\n",
        "            objects_list = tf.unstack(state, N_OBJECTS, 1)\n",
        "            RM_inputs = []\n",
        "            for i in range(n_relations):\n",
        "                obj_1 = objects_list[int(i / (N_OBJECTS - 1))]\n",
        "                obj_2 = objects_list[int(i % (N_OBJECTS - 1))]\n",
        "                RM_inputs.append(tf.concat([obj_1, obj_2], 1))\n",
        "            RM_inputs = tf.concat(RM_inputs, 0)\n",
        "\n",
        "            # relation network\n",
        "            h_1 = Linear('h_1', RM_inputs, CORE_STATE_DIMS, bias_zero_init=False)\n",
        "            h_2 = Linear('h_2', h_1, CORE_STATE_DIMS, skip_connection=True, bias_zero_init=False)\n",
        "            h_3 = Linear('h_3', h_2, CORE_STATE_DIMS, skip_connection=True, bias_zero_init=False, activation_fn=None)\n",
        "            RM_outputs = tf.split(h_3, n_relations, 0)\n",
        "\n",
        "            # accumulate all pairwise effects in an object\n",
        "            acc_relations = np.zeros(N_OBJECTS, dtype=object)\n",
        "            for i in range(N_OBJECTS):\n",
        "                for j in range(N_OBJECTS - 1):\n",
        "                    acc_relations[i] += RM_outputs[i * (N_OBJECTS - 1) + j]\n",
        "            acc_relations = tf.stack(list(acc_relations), 1)\n",
        "        ##############################################################################################\n",
        "        ###                                  End of your code                                      ###\n",
        "        ##############################################################################################\n",
        "        \n",
        "        ### Fuse Self-Dynamics and Relations ###\n",
        "        fused_obj_states = SD_outputs + acc_relations\n",
        "\n",
        "        ### Affector ###\n",
        "        with tf.variable_scope('affector'):\n",
        "            AFF_inputs = tf.reshape(fused_obj_states, [-1, CORE_STATE_DIMS])\n",
        "            h_1 = Linear('h_1', AFF_inputs, CORE_STATE_DIMS, skip_connection=True, bias_zero_init=False)\n",
        "            h_2 = Linear('h_2', h_1, CORE_STATE_DIMS, skip_connection=True, bias_zero_init=False)\n",
        "            h_3 = Linear('h_3', h_2, CORE_STATE_DIMS, skip_connection=True, bias_zero_init=False, activation_fn=None)\n",
        "            AFF_outputs = tf.reshape(h_3, [-1, N_OBJECTS, CORE_STATE_DIMS])\n",
        "\n",
        "        ### Output Module ###\n",
        "        with tf.variable_scope('output'):\n",
        "            OUT_inputs = tf.reshape(tf.concat([state, AFF_outputs], 2), [-1, STATE_DIMS + CORE_STATE_DIMS])\n",
        "            h_1 = Linear('h_1', OUT_inputs, CORE_STATE_DIMS, bias_zero_init=False)\n",
        "            h_2 = Linear('h_2', h_1, CORE_STATE_DIMS, bias_zero_init=False, activation_fn=None)\n",
        "            OUT_outputs = tf.reshape(h_2, [-1, N_OBJECTS, STATE_DIMS])\n",
        "\n",
        "    return OUT_outputs\n",
        "\n",
        "\n",
        "def DP(states_list):\n",
        "    with tf.variable_scope('DP'):\n",
        "        ### Pairwise Relation and Self-Dynamics of objects ###\n",
        "        with tf.variable_scope('core'):\n",
        "            relations_list = []\n",
        "            for i in TEMPORAL_OFFSETS:\n",
        "                relations_list.append(core(states_list[i - 1], N_FRAMES - 1 + i))\n",
        "            relations = tf.concat(relations_list, 2)\n",
        "            relations = tf.reshape(relations, [-1, STATE_DIMS * len(TEMPORAL_OFFSETS)])\n",
        "\n",
        "        ### Aggregator ###\n",
        "        with tf.variable_scope('aggregator'):\n",
        "            h_1 = Linear('h_1', relations, DP_STATE_DIMS, bias_zero_init=False)\n",
        "            h_2 = Linear('h_2', h_1, STATE_DIMS, activation_fn=None, bias_zero_init=False)\n",
        "            h_2 = tf.reshape(h_2, [-1, N_OBJECTS, STATE_DIMS])\n",
        "\n",
        "    return h_2\n",
        "\n",
        "\n",
        "##############################################################################################\n",
        "###    Given a short segment of sequence, please perform rollout using dynamic predictor   ###\n",
        "##############################################################################################\n",
        "def rollout_DP(prev_out, cur_in):\n",
        "    states_list = tf.unstack(prev_out, N_FRAMES - 2, 0)\n",
        "    state_pred = DP(states_list)\n",
        "    out = tf.stack(states_list[1:] + [state_pred], 0)\n",
        "    return out\n",
        "\n",
        "elems = tf.identity(tf.Variable(tf.zeros([ROLLOUT_NUM], dtype=tf.float32)))\n",
        "rollout_init = tf.stack(sliding_window_list, 0)\n",
        "rollout = tf.scan(rollout_DP, elems, initializer=rollout_init)\n",
        "##############################################################################################\n",
        "###                                  End of your code                                      ###\n",
        "##############################################################################################\n",
        "state_pred = tf.unstack(rollout, N_FRAMES - 2, 1)[-1] # obtain the last step in the rollout\n",
        "state_pred = tf.stack(tf.unstack(state_pred, BATCH_SIZE, 1), 0) # change axis of rollout step and batch\n",
        "state_pred = tf.reshape(state_pred, [-1, N_OBJECTS, STATE_DIMS])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZsnkONvD8Gie",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## State Decoder\n",
        "The state decoder is simply <font color='blue'>a linear layer with input size $L_{code}$ and output size 4</font> (for position / velocity vector). This linear layer is applied independently to each slot of the state code.The state decoder is applied to both encoded state codes in current time steps (for reconstruction loss) and the predicted future states codes (for prediction loss)."
      ]
    },
    {
      "metadata": {
        "id": "VVNVSsAkdpIh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\" State Decoder \"\"\"\n",
        "def SD(states):\n",
        "    SD_inputs = tf.reshape(states, [-1, STATE_DIMS])\n",
        "    h_1 = Linear('h_1', SD_inputs, OUTPUT_DIMS, initializer=tf.truncated_normal_initializer(stddev=0.1), activation_fn=None)\n",
        "    SD_outputs = tf.reshape(h_1, [-1, N_OBJECTS, OUTPUT_DIMS])\n",
        "\n",
        "    return SD_outputs\n",
        "\n",
        "\n",
        "# decode all states\n",
        "all_states = tf.concat(sliding_window_list + [state_pred], 0)\n",
        "output_states = SD(all_states)\n",
        "\n",
        "# encoded-decoded current states\n",
        "current_states_pred = []\n",
        "for i in range(N_FRAMES - 2):\n",
        "    s_pred = tf.slice(output_states, [BATCH_SIZE * i, 0, 0], [BATCH_SIZE, -1, -1])\n",
        "    current_states_pred.append(s_pred)\n",
        "\n",
        "# predicted future states\n",
        "future_states_pred = tf.slice(output_states, [BATCH_SIZE * (N_FRAMES - 2), 0, 0], [-1, -1, -1])\n",
        "future_states_pred = tf.reshape(future_states_pred, [BATCH_SIZE, ROLLOUT_NUM, N_OBJECTS, OUTPUT_DIMS])\n",
        "nstep_future_states_pred = tf.unstack(future_states_pred, ROLLOUT_NUM, 1)[:FUTURE_N_STEPS]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dh2_Gfpx8JK9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss and Trainer"
      ]
    },
    {
      "metadata": {
        "id": "BH0wWAh5mx9H",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Loss and Trainer \"\"\"\n",
        "### Losses ###\n",
        "# visual encoder loss\n",
        "ve_loss = tf.reduce_mean(tf.square(current_label_list[0] - current_states_pred[0]))\n",
        "for i in range(1, N_FRAMES - 2):\n",
        "    ve_loss += tf.reduce_mean(tf.square(current_label_list[i] - current_states_pred[i]))\n",
        "ve_loss = ve_loss / (N_FRAMES - 2)\n",
        "\n",
        "# dynamics predictor loss, posed on predicted future n-step output states\n",
        "dp_loss = discount_factor * tf.reduce_mean(tf.square(future_label_list[0] - nstep_future_states_pred[0]))\n",
        "for i in range(1, FUTURE_N_STEPS):\n",
        "    dp_loss += ((discount_factor ** (i + 1)) * tf.reduce_mean(tf.square(future_label_list[i] - nstep_future_states_pred[i])))\n",
        "dp_loss = dp_loss / FUTURE_N_STEPS\n",
        "\n",
        "loss = ve_loss + dp_loss\n",
        "\n",
        "### Trainer ###\n",
        "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
        "trainer = optimizer.minimize(loss)\n",
        "\n",
        "### Session and Initializer ###\n",
        "tf_config = tf.ConfigProto()\n",
        "tf_config.gpu_options.allow_growth = True\n",
        "tf_config.allow_soft_placement = True\n",
        "tf_config.log_device_placement = False\n",
        "sess = tf.Session(config=tf_config)\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tc7Oeyhl8MSy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and Testing\n",
        "<img src=\"https://i.imgur.com/Pux22wI.png\" width=\"400\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img src=\"https://i.imgur.com/iprRS3T.gif\" width=\"200\">\n",
        "<img src=\"https://i.imgur.com/7DaRQeq.gif\" width=\"200\">"
      ]
    },
    {
      "metadata": {
        "id": "NLKZ7D2Gw2E6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "outputId": "f54a38a6-e9a6-436c-ae51-2d83e10c67ab",
        "executionInfo": {
          "status": "error",
          "timestamp": 1533396156087,
          "user_tz": -480,
          "elapsed": 1859,
          "user": {
            "displayName": "Tsun Hsuan Wang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112844842626206939059"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Training / Testing \"\"\"\n",
        "assert PSEUDO == False, 'Please turn off \\'PSEUDO\\' for training or testing'\n",
        "saver = tf.train.Saver(max_to_keep=5)\n",
        "if MODE == 'train':\n",
        "    discount_factor_value = 1\n",
        "    for ep in range(MAX_EPOCHS):\n",
        "        tic = time.time()\n",
        "        loss_list = []\n",
        "        ve_loss_list = []\n",
        "        dp_loss_list = []\n",
        "        for it in range(int(dset.size / BATCH_SIZE)):\n",
        "            feed_dict = {discount_factor: discount_factor_value,\n",
        "                         x_coor: dset.x_coor,\n",
        "                         y_coor: dset.y_coor}\n",
        "            fetch = [loss, ve_loss, dp_loss, trainer]\n",
        "            loss_, ve_loss_, dp_loss_, _ = sess.run(fetch, feed_dict)\n",
        "            loss_list.append(loss_)\n",
        "            ve_loss_list.append(ve_loss_)\n",
        "            dp_loss_list.append(dp_loss_)\n",
        "        toc = time.time()\n",
        "        avg_loss = np.mean(loss_list)\n",
        "        avg_ve_loss = np.mean(ve_loss_list)\n",
        "        avg_dp_loss = np.mean(dp_loss_list)\n",
        "        print('Epoch [{}/{}]: loss = {:.2f}, ve_loss = {:.2f}, dp_loss = {:.2f}, epoch_time = {:.2f}'.\\\n",
        "              format(ep, MAX_EPOCHS, avg_loss, avg_ve_loss, avg_dp_loss, toc - tic))\n",
        "        \n",
        "        if ((ep % 10) == 0 and ep != 0) or ep == (MAX_EPOCHS - 1):\n",
        "            ckpt_path = './model-ep{:05}.ckpt'.format(ep)\n",
        "            print('Save checkpoint to {}'.format(ckpt_path))\n",
        "            saver.save(sess, ckpt_path)\n",
        "        \n",
        "else:\n",
        "    # load model\n",
        "    saver.restore(sess, './model-ep00030.ckpt')\n",
        "\n",
        "    # prediction\n",
        "    feed_dict = {x_coor: dset.x_coor,\n",
        "                 y_coor: dset.y_coor}\n",
        "    fetch = [future_states_pred, dset._current_labels, dset._future_labels]\n",
        "    future_states_pred_, current_labels_, future_labels_ = sess.run(fetch, feed_dict)\n",
        "    xy_origin = future_labels_[:(SIM_FRAMES - (N_FRAMES + FUTURE_N_STEPS) + 1 - (N_FRAMES - 2) + 1), 0, :, 0:2]\n",
        "    vel = future_states_pred_[0, :, :, 2:4]\n",
        "    xy_estimate = np.zeros((ROLLOUT_NUM, N_OBJECTS, 2), dtype=float)\n",
        "    xy_estimate[0] = current_labels_[3][3][:, :2] + vel[0] * 0.01\n",
        "    for i in range(1, future_states_pred_.shape[0]):\n",
        "        xy_estimate[i] = xy_estimate[i - 1] + vel[i] * 0.01\n",
        "\n",
        "    # visualize\n",
        "    amp_factor = 9\n",
        "    fig, axes = plt.subplots(1, 2)\n",
        "    color = ['r', 'b', 'g', 'y', 'm', 'c']\n",
        "    for ax in axes:\n",
        "        ax.set_xlim(-100 * amp_factor, 100 * amp_factor)\n",
        "        ax.set_ylim(-100 * amp_factor, 100 * amp_factor)\n",
        "        ax.axis('off')\n",
        "    axes[0].set_title('Ground Truth')\n",
        "    axes[1].set_title('Prediction')\n",
        "    scat_ori = []\n",
        "    scat_est = []\n",
        "    for j in range(N_OBJECTS):\n",
        "        scat_ori.append(axes[0].scatter(xy_origin[0, j, 1] * amp_factor, \n",
        "                                        xy_origin[0, j, 0] * amp_factor, \n",
        "                                        c=color[j % len(color)], s=(6 * amp_factor)))\n",
        "        scat_est.append(axes[1].scatter(xy_estimate[0, j, 1] * amp_factor, \n",
        "                                        xy_estimate[0, j, 0] * amp_factor, \n",
        "                                        c=color[j % len(color)], s=(6 * amp_factor)))\n",
        "    def update(i):\n",
        "        for j in range(N_OBJECTS):\n",
        "            scat_ori[j].set_offsets([xy_origin[i, j, 1] * amp_factor, \n",
        "                                     xy_origin[i, j, 0] * amp_factor])\n",
        "            scat_est[j].set_offsets([xy_estimate[i, j, 1] * amp_factor, \n",
        "                                     xy_estimate[i, j, 0] * amp_factor])\n",
        "    anim_frames = min(xy_origin.shape[0], xy_estimate.shape[0])\n",
        "    anim = FuncAnimation(fig, update, frames=anim_frames, interval=500, repeat=True)\n",
        "    #HTML(anim.to_html5_video())\n",
        "    plt.show()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model-ep00030.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-0fed93b39bd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0manim_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_origin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy_estimate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0manim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFuncAnimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manim_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html5_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mto_html5_video\u001b[0;34m(self, embed_limit)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 \u001b[0;31m# We create a writer manually so that we can get the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m                 \u001b[0;31m# appropriate size for the tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m                 \u001b[0mWriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwriters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'animation.writer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m                 writer = Writer(codec='h264',\n\u001b[1;32m   1411\u001b[0m                                 \u001b[0mbitrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'animation.bitrate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavail\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No MovieWriters available!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavail\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0mwriters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMovieWriterRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'ffmpeg'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEHCAYAAACgHI2PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8c8037eda0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XTEqHW8i9fpC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "- Battaglia P, Pascanu R, Lai M, Rezende DJ. [Interaction networks for learning about objects, relations and physics](https://arxiv.org/abs/1612.00222). NIPS 2016\n",
        "- Santoro A, Raposo D, Barrett DG, Malinowski M, Pascanu R, Battaglia P, Lillicrap T. [A simple neural network module for relational reasoning](https://arxiv.org/abs/1706.01427). NIPS 2017\n",
        "- Watters N, Tacchetti A, Weber T, Pascanu R, Battaglia P, Zoran D. [Visual interaction networks](https://arxiv.org/abs/1706.01433). arXiv 2017\n",
        "- [A neural approach to relational reasoning](https://deepmind.com/blog/neural-approach-relational-reasoning/)\n",
        "- [visual-interaction-networks_tensorflow](https://github.com/jaesik817/visual-interaction-networks_tensorflow)\n"
      ]
    },
    {
      "metadata": {
        "id": "n1WkotBN9j77",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Solutions\n",
        "- <font size=\"4\">**Temporal convolution in IPE**:</font><font color=\"white\">\n",
        "    \n",
        "```\n",
        "fpairs = tf.concat(fpairs_list, 0)\n",
        "inputs = tf.concat([fpairs, x, y], 3)\n",
        "\n",
        "h_0 = MaxPool2d('mp_0', Conv2d('h_0', inputs, N_FILTERS))\n",
        "h_1 = MaxPool2d('mp_1', Conv2d('h_1', h_0, N_FILTERS, skip_connection=True))\n",
        "h_2 = MaxPool2d('mp_2', Conv2d('h_2', h_1, N_FILTERS, skip_connection=True))\n",
        "h_3 = MaxPool2d('mp_3', Conv2d('h_3', h_2, N_FILTERS, skip_connection=True))\n",
        "h_4 = MaxPool2d('mp_4', Conv2d('h_4', h_3, N_FILTERS, skip_connection=True))\n",
        "\n",
        "# separate along time axis\n",
        "pairs = tf.reshape(h_4, [-1, N_FILTERS])\n",
        "p_list = []\n",
        "for i in range(N_FRAMES - 1):\n",
        "    p_list.append(tf.slice(pairs, [BATCH_SIZE * i, 0], [BATCH_SIZE, -1]))\n",
        "```\n",
        "</font>\n",
        "- <font size=\"4\">**Self-dynamics module:**</font><font color=\"white\">\n",
        "\n",
        "```\n",
        "### Self-Dynamics Module ###\n",
        "with tf.variable_scope('self_dynamics'):\n",
        "    SD_inputs = tf.reshape(state, [-1, STATE_DIMS])\n",
        "    h_1 = Linear('h_1', SD_inputs, CORE_STATE_DIMS, bias_zero_init=False)\n",
        "    h_2 = Linear('h_2', h_1, CORE_STATE_DIMS, skip_connection=True, bias_zero_init=False, activation_fn=None)\n",
        "```\n",
        "</font>\n",
        "- <font size=\"4\">**Relation module:**</font><font color=\"white\">\n",
        "\n",
        "```\n",
        "n_relations = int(N_OBJECTS * (N_OBJECTS - 1))\n",
        "with tf.variable_scope('relation_module'):\n",
        "    # collect object pairs as inputs\n",
        "    objects_list = tf.unstack(state, N_OBJECTS, 1)\n",
        "    RM_inputs = []\n",
        "    for i in range(n_relations):\n",
        "        obj_1 = objects_list[int(i / (N_OBJECTS - 1))]\n",
        "        obj_2 = objects_list[int(i % (N_OBJECTS - 1))]\n",
        "        RM_inputs.append(tf.concat([obj_1, obj_2], 1))\n",
        "    RM_inputs = tf.concat(RM_inputs, 0)\n",
        "\n",
        "    # relation network\n",
        "    h_1 = Linear('h_1', RM_inputs, CORE_STATE_DIMS, bias_zero_init=False)\n",
        "    h_2 = Linear('h_2', h_1, CORE_STATE_DIMS, skip_connection=True, bias_zero_init=False)\n",
        "    h_3 = Linear('h_3', h_2, CORE_STATE_DIMS, skip_connection=True, bias_zero_init=False, activation_fn=None)\n",
        "    RM_outputs = tf.split(h_3, n_relations, 0)\n",
        "\n",
        "    # accumulate all pairwise effects in an object\n",
        "    acc_relations = np.zeros(N_OBJECTS, dtype=object)\n",
        "    for i in range(N_OBJECTS):\n",
        "        for j in range(N_OBJECTS - 1):\n",
        "            acc_relations[i] += RM_outputs[i * (N_OBJECTS - 1) + j]\n",
        "    acc_relations = tf.stack(list(acc_relations), 1)\n",
        "```\n",
        "</font>\n",
        "- <font size=\"4\">**Rollout:**</font><font color=\"white\">\n",
        "\n",
        "```\n",
        "def rollout_DP(prev_out, cur_in):\n",
        "    states_list = tf.unstack(prev_out, N_FRAMES - 2, 0)\n",
        "    state_pred = DP(states_list)\n",
        "    out = tf.stack(states_list[1:] + [state_pred], 0)\n",
        "    return out\n",
        "\n",
        "elems = tf.identity(tf.Variable(tf.zeros([ROLLOUT_NUM], dtype=tf.float32)))\n",
        "rollout_init = tf.stack(sliding_window_list, 0)\n",
        "rollout = tf.scan(rollout_DP, elems, initializer=rollout_init)\n",
        "```\n",
        "</font>\n",
        "\n"
      ]
    }
  ]
}